{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73a086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS DEFINITIONS FOR NEURAL NETWORKS USED IN DEEP GALERKIN METHOD\n",
    "\n",
    "#%% import needed packages\n",
    "import tensorflow as tf\n",
    "\n",
    "#%% LSTM-like layer used in DGM (see Figure 5.3 and set of equations on p. 45) - modification of Keras layer class\n",
    "\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim (int):       dimensionality of input data\n",
    "            output_dim (int):      number of outputs for LSTM layers\n",
    "            trans1, trans2 (str):  activation functions used inside the layer; \n",
    "                                   one of: \"tanh\" (default), \"relu\" or \"sigmoid\"\n",
    "        \n",
    "        Returns: customized Keras layer object used as intermediate layers in DGM\n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of LSTMLayer)\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        # add properties for layer including activation functions used inside the layer  \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.sigmoid\n",
    "        \n",
    "        ### define LSTM layer parameters (use Xavier initialization)\n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_weight(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        self.Ug = self.add_weight(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        self.Ur = self.add_weight(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        self.Uh = self.add_weight(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_weight(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        self.Wg = self.add_weight(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        self.Wr = self.add_weight(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        self.Wh = self.add_weight(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_weight(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_weight(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_weight(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_weight(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    # main function to be called \n",
    "    def call(self, S, X):\n",
    "        '''Compute output of a LSTMLayer for a given inputs S,X .    \n",
    "        Args:            \n",
    "            S: output of previous layer\n",
    "            X: data input\n",
    "        \n",
    "        Returns: customized Keras layer object used as intermediate layers in DGM\n",
    "        '''   \n",
    "        \n",
    "        # compute components of LSTM layer output (note H uses a separate activation function)\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        # compute LSTM layer output\n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new\n",
    "\n",
    "#%% Fully connected (dense) layer - modification of Keras layer class\n",
    "   \n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation = \"relu\"):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim:       dimensionality of input data\n",
    "            output_dim:      number of outputs for dense layer\n",
    "            transformation:  activation function used inside the layer; using\n",
    "                             None is equivalent to the identity map \n",
    "        \n",
    "        Returns: customized Keras (fully connected) layer object \n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of DenseLayer)\n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        ### define dense layer parameters (use Xavier initialization)\n",
    "        # w vectors (weighting vectors for output of previous layer)\n",
    "        self.W = self.add_weight(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_weight(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.nn.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    # main function to be called \n",
    "    def call(self,X):\n",
    "        '''Compute output of a dense layer for a given input X \n",
    "        Args:                        \n",
    "            X: input to layer            \n",
    "        '''\n",
    "        \n",
    "        # compute dense layer output\n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S\n",
    "\n",
    "#%% Neural network architecture used in DGM - modification of Keras Model class\n",
    "    \n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans = None):\n",
    "        '''\n",
    "        Args:\n",
    "            layer_width: \n",
    "            n_layers:    number of intermediate LSTM layers\n",
    "            input_dim:   spaital dimension of input data (EXCLUDES time dimension)\n",
    "            final_trans: transformation used in final layer\n",
    "        \n",
    "        Returns: customized Keras model object representing DGM neural network\n",
    "        '''  \n",
    "        \n",
    "        # create an instance of a Model object (call initialize function of superclass of DGMNet)\n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        # define initial layer as fully connected \n",
    "        # NOTE: to account for time inputs we use input_dim+1 as the input dimensionality\n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"relu\")\n",
    "        \n",
    "        # define intermediate LSTM layers\n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1))\n",
    "        \n",
    "        # define final layer as fully connected with a single output (function value)\n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    \n",
    "    # main function to be called  \n",
    "    def call(self,t,x):\n",
    "        '''            \n",
    "        Args:\n",
    "            t: sampled time inputs \n",
    "            x: sampled space inputs\n",
    "        Run the DGM model and obtain fitted function value at the inputs (t,x)                \n",
    "        '''  \n",
    "        \n",
    "        # define input vector as time-space pairs\n",
    "        X = tf.concat([t,x],1)\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419.50815 3.0096574 416.4985 0\n",
      "119.01975 5.522969 113.49678 1\n",
      "52.966267 8.19524 44.771027 2\n",
      "13.954557 3.042294 10.912263 3\n",
      "10.092353 2.0054266 8.086926 4\n",
      "6.2805233 0.9198457 5.3606777 5\n",
      "4.2038517 0.7584377 3.4454138 6\n",
      "2.3941674 0.4742949 1.9198724 7\n",
      "1.649305 0.32843292 1.3208721 8\n",
      "1.1903423 0.24340144 0.9469409 9\n",
      "1.8611927 0.18518984 1.6760029 10\n",
      "1.6733689 0.54754245 1.1258265 11\n",
      "0.95376146 0.48024324 0.47351825 12\n",
      "0.6130662 0.25192282 0.36114335 13\n",
      "0.4578046 0.15405291 0.30375168 14\n",
      "2.0917616 0.5213403 1.5704213 15\n",
      "1.2582524 0.26140377 0.9968486 16\n",
      "0.91469324 0.27871898 0.6359743 17\n",
      "0.64350986 0.17345268 0.47005722 18\n",
      "0.23228261 0.080131985 0.15215063 19\n",
      "0.5586293 0.19247623 0.36615303 20\n",
      "0.3800009 0.12400184 0.25599906 21\n",
      "0.43287802 0.14712922 0.2857488 22\n",
      "0.3834461 0.14031103 0.24313505 23\n",
      "0.31944692 0.10851484 0.21093208 24\n",
      "0.24261558 0.08389486 0.15872072 25\n",
      "0.19683766 0.08523831 0.111599356 26\n",
      "0.22173776 0.092833936 0.12890382 27\n",
      "0.09910044 0.049171485 0.04992896 28\n",
      "0.16125828 0.07548357 0.085774705 29\n",
      "0.15302351 0.061184913 0.091838606 30\n",
      "0.17366502 0.055638604 0.11802642 31\n",
      "0.07798158 0.03423778 0.0437438 32\n",
      "0.15043575 0.06308374 0.087352 33\n",
      "0.08624636 0.042250436 0.04399592 34\n",
      "0.06544287 0.03514106 0.030301811 35\n",
      "0.05865957 0.03421953 0.02444004 36\n",
      "0.10357503 0.039631173 0.063943855 37\n",
      "0.14147365 0.06322784 0.07824582 38\n",
      "0.10915211 0.039606083 0.06954603 39\n",
      "0.08588265 0.043546144 0.04233651 40\n",
      "0.3575908 0.17983471 0.17775609 41\n",
      "0.14190203 0.07821834 0.06368369 42\n",
      "0.07820998 0.04252511 0.035684876 43\n",
      "0.07568616 0.03719051 0.03849564 44\n",
      "0.046740077 0.027535457 0.01920462 45\n",
      "0.053979285 0.028570117 0.025409168 46\n",
      "0.06383547 0.03090833 0.03292714 47\n",
      "0.060178183 0.030601386 0.029576797 48\n",
      "0.049493693 0.022532254 0.026961436 49\n",
      "0.044552997 0.0227914 0.021761594 50\n",
      "0.06658522 0.024862504 0.04172272 51\n",
      "0.042337514 0.023069512 0.019268002 52\n",
      "0.041465633 0.020047057 0.021418577 53\n",
      "0.32431322 0.09653276 0.22778048 54\n",
      "0.057052173 0.031855833 0.025196338 55\n",
      "0.05281593 0.025018416 0.027797513 56\n",
      "0.032631725 0.019597493 0.013034232 57\n",
      "0.026460027 0.015522647 0.010937381 58\n",
      "0.030043568 0.016946148 0.013097421 59\n",
      "0.07095438 0.02958998 0.0413644 60\n",
      "0.045896735 0.021882894 0.024013842 61\n",
      "0.040963233 0.018439446 0.022523785 62\n",
      "0.12455245 0.035539046 0.089013405 63\n",
      "0.21524498 0.035998635 0.17924634 64\n",
      "0.0372087 0.020568866 0.016639832 65\n",
      "0.027756471 0.014986904 0.012769566 66\n",
      "0.0658219 0.0189468 0.046875097 67\n",
      "0.06634656 0.02652861 0.03981795 68\n",
      "0.10478367 0.039057557 0.06572611 69\n",
      "0.015907958 0.012577549 0.0033304105 70\n",
      "0.114751935 0.03525267 0.07949927 71\n",
      "0.084448725 0.03408287 0.050365858 72\n",
      "0.04367684 0.022796107 0.020880733 73\n",
      "0.026335478 0.016712265 0.009623212 74\n",
      "0.032300435 0.01935133 0.012949106 75\n",
      "0.032856055 0.017458549 0.015397508 76\n",
      "0.037545912 0.020590942 0.01695497 77\n",
      "0.06381462 0.02148488 0.04232974 78\n",
      "0.051578626 0.024712453 0.026866173 79\n",
      "0.04057066 0.01660557 0.023965092 80\n",
      "0.029254247 0.018679455 0.0105747925 81\n",
      "0.1627076 0.041513648 0.121193945 82\n",
      "0.033171408 0.020865604 0.012305805 83\n",
      "0.036811665 0.020436006 0.016375659 84\n",
      "0.0732873 0.02000832 0.05327898 85\n",
      "0.026185181 0.016126448 0.010058732 86\n",
      "0.04899942 0.018873269 0.030126153 87\n",
      "0.05107382 0.018114986 0.032958835 88\n",
      "0.03499876 0.014052551 0.02094621 89\n",
      "0.024259668 0.014124852 0.010134815 90\n",
      "0.031593017 0.015943758 0.01564926 91\n",
      "0.019993015 0.013250865 0.00674215 92\n",
      "0.036120154 0.015803037 0.020317115 93\n",
      "0.028568622 0.014691152 0.01387747 94\n",
      "0.025946692 0.013681052 0.012265639 95\n",
      "0.078710884 0.02386485 0.05484603 96\n",
      "0.06463599 0.019983398 0.044652596 97\n",
      "0.12147255 0.03093026 0.090542294 98\n",
      "0.042914588 0.01940057 0.023514017 99\n",
      "0.59831166 0.23889461 0.35941705 100\n",
      "0.60229975 0.2439786 0.35832116 101\n",
      "0.10167077 0.034662712 0.067008056 102\n",
      "0.05717448 0.022729645 0.03444484 103\n",
      "0.028550614 0.01644056 0.0121100545 104\n",
      "0.02504335 0.014326315 0.010717034 105\n",
      "0.045973994 0.017647969 0.028326025 106\n",
      "0.025311783 0.013718684 0.011593098 107\n",
      "0.026737986 0.01256469 0.014173296 108\n",
      "0.07096058 0.02754614 0.043414444 109\n",
      "0.099617116 0.014477932 0.085139185 110\n",
      "0.43773243 0.11413225 0.32360017 111\n",
      "0.055054568 0.031248976 0.02380559 112\n",
      "0.03258101 0.018908795 0.013672214 113\n",
      "0.03731049 0.019860666 0.017449822 114\n",
      "0.028350629 0.015984502 0.012366128 115\n",
      "0.02386143 0.013007536 0.010853895 116\n",
      "0.014476582 0.00927457 0.005202012 117\n",
      "0.017357472 0.009870596 0.007486877 118\n",
      "0.010691214 0.008340483 0.0023507313 119\n",
      "0.01993022 0.008005336 0.011924883 120\n",
      "0.021371843 0.009004228 0.0123676155 121\n",
      "0.022656543 0.009607872 0.013048671 122\n",
      "0.020505507 0.009305765 0.011199742 123\n",
      "0.015676763 0.007258168 0.0084185945 124\n",
      "1.1728963 0.08722612 1.0856701 125\n",
      "0.052228462 0.030609211 0.021619251 126\n",
      "0.03192158 0.016143506 0.015778072 127\n",
      "0.023774954 0.01093544 0.012839514 128\n",
      "0.022877738 0.009493307 0.013384432 129\n",
      "0.0069590537 0.005515831 0.0014432227 130\n",
      "0.0108577255 0.006180517 0.0046772086 131\n",
      "0.017227069 0.006795684 0.010431385 132\n",
      "0.0133217955 0.006217194 0.0071046017 133\n",
      "0.014780116 0.0072219684 0.007558148 134\n",
      "0.00925483 0.0051652757 0.004089554 135\n",
      "0.15215175 0.025334092 0.12681766 136\n",
      "0.017141987 0.007117125 0.010024861 137\n",
      "0.010432363 0.006333481 0.004098882 138\n",
      "0.021777257 0.0069005955 0.014876661 139\n",
      "0.011051758 0.0054035876 0.0056481706 140\n",
      "0.014032288 0.006429199 0.0076030893 141\n",
      "0.015695184 0.0060875174 0.009607668 142\n",
      "0.23934716 0.079121254 0.16022591 143\n",
      "0.08896702 0.02771516 0.06125186 144\n",
      "0.029224321 0.011334526 0.017889796 145\n",
      "0.02715609 0.005498666 0.021657424 146\n",
      "0.01406737 0.006474533 0.007592838 147\n",
      "0.014803487 0.0050968407 0.009706646 148\n",
      "0.01459778 0.005789441 0.008808339 149\n",
      "0.7131696 0.1085945 0.6045751 150\n",
      "0.08911519 0.05754893 0.031566255 151\n",
      "0.047659513 0.015614544 0.03204497 152\n",
      "0.022108354 0.009453812 0.012654543 153\n",
      "0.021977268 0.00790081 0.014076457 154\n",
      "0.01775152 0.0062764217 0.011475099 155\n",
      "0.009557769 0.004675603 0.004882166 156\n",
      "0.01071928 0.0051640538 0.005555226 157\n",
      "0.006915983 0.0042439117 0.002672071 158\n",
      "0.014289119 0.0055375025 0.008751617 159\n",
      "0.012390709 0.006393077 0.005997632 160\n",
      "0.004442999 0.0034979177 0.00094508164 161\n",
      "0.016984329 0.0062837014 0.010700626 162\n",
      "0.023200123 0.011750729 0.011449394 163\n",
      "0.011761932 0.0051649082 0.006597023 164\n",
      "0.021250002 0.008199182 0.013050821 165\n",
      "0.0073105427 0.003336213 0.0039743297 166\n",
      "0.011578455 0.0047481842 0.0068302704 167\n",
      "0.0077738487 0.004163191 0.0036106575 168\n",
      "0.009044986 0.0043130415 0.0047319448 169\n",
      "0.011432554 0.0048162127 0.006616341 170\n",
      "0.026895216 0.009081313 0.017813902 171\n",
      "0.011866854 0.004000604 0.007866249 172\n",
      "0.011998534 0.0064796805 0.005518854 173\n",
      "0.17666817 0.041284867 0.13538331 174\n",
      "0.94062275 0.10368384 0.8369389 175\n",
      "0.14233372 0.07247478 0.06985893 176\n",
      "0.032494187 0.017235152 0.015259035 177\n",
      "0.025539849 0.013672661 0.011867187 178\n",
      "0.015307061 0.008146793 0.007160269 179\n",
      "0.013927354 0.0076068398 0.0063205133 180\n",
      "0.0097491965 0.0061348574 0.0036143395 181\n",
      "0.01206496 0.006311871 0.005753089 182\n",
      "0.010280977 0.006508955 0.0037720224 183\n",
      "0.015302269 0.007467396 0.007834873 184\n",
      "0.013226412 0.005208561 0.008017851 185\n",
      "0.0131356865 0.0048953514 0.008240335 186\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT FOR SOLVING THE BLACK-SCHOLES EQUATION FOR A EUROPEAN CALL OPTION \n",
    "\n",
    "#%% import needed packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as spstats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#%% Parameters \n",
    "\n",
    "# Option parameters\n",
    "r = 0.05           # Interest rate\n",
    "sigma = 0.25       # Volatility\n",
    "K = 50             # Strike\n",
    "T = 1              # Terminal time\n",
    "S0 = 0.5           # Initial price\n",
    "\n",
    "# Solution parameters (domain on which to solve PDE)\n",
    "t_low = 0 #+ 1e-10    # time lower bound\n",
    "S_low = 0 + 1e-10  # spot price lower bound\n",
    "S_high = 2*K         # spot price upper bound\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages  = 200   # number of times to resample new time-space domain points\n",
    "steps_per_sample = 20    # number of SGD steps to take before re-sampling\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_interior = 1000\n",
    "nSim_terminal = 100\n",
    "S_multiplier  = 1.5   # multiplier for oversampling i.e. draw S from [S_low, S_high * S_multiplier]\n",
    "\n",
    "# Plot options\n",
    "n_plot = 41  # Points on plot grid for each dimension\n",
    "\n",
    "# Save options\n",
    "saveOutput = True\n",
    "saveName   = 'BlackScholes_EuropeanCall'\n",
    "saveFigure = True\n",
    "figureName = 'BlackScholes_EuropeanCall'\n",
    "\n",
    "#%% Black-Scholes European call price\n",
    "\n",
    "def BlackScholesCall(S, K, r, sigma, t):\n",
    "    ''' Analytical solution for European call option price under Black-Scholes model \n",
    "    \n",
    "    Args:\n",
    "        S:     spot price\n",
    "        K:     strike price\n",
    "        r:     risk-free interest rate\n",
    "        sigma: volatility\n",
    "        t:     time\n",
    "    ''' \n",
    "    callPrice = np.maximum(S-K, 0)\n",
    "\n",
    "    if (T!=t):\n",
    "        d1 = (np.log(S/K) + (r + sigma**2 / 2) * (T-t))/(sigma * np.sqrt(T-t))\n",
    "        d2 = d1 - (sigma * np.sqrt(T-t))\n",
    "        callPrice = S * spstats.norm.cdf(d1) - K * np.exp(-r * (T-t)) * spstats.norm.cdf(d2)\n",
    "    \n",
    "    return callPrice\n",
    "\n",
    "#%% Sampling function - randomly sample time-space pairs \n",
    "\n",
    "def sampler(nSim_interior, nSim_terminal):\n",
    "    ''' Sample time-space points from the function's domain; points are sampled\n",
    "        uniformly on the interior of the domain, at the initial/terminal time points\n",
    "        and along the spatial boundary at different time points. \n",
    "    \n",
    "    Args:\n",
    "        nSim_interior: number of space points in the interior of the function's domain to sample \n",
    "        nSim_terminal: number of space points at terminal time to sample (terminal condition)\n",
    "    ''' \n",
    "    \n",
    "    # Sampler #1: domain interior\n",
    "    t_interior = np.random.uniform(low=t_low, high=T, size=[nSim_interior, 1])\n",
    "    S_interior = np.random.uniform(low=S_low, high=S_high*S_multiplier, size=[nSim_interior, 1])\n",
    "\n",
    "    # Sampler #2: spatial boundary\n",
    "        # no spatial boundary condition for this problem\n",
    "    \n",
    "    # Sampler #3: initial/terminal condition\n",
    "    t_terminal = T * np.ones((nSim_terminal, 1))\n",
    "    S_terminal = np.random.uniform(low=S_low, high=S_high*S_multiplier, size = [nSim_terminal, 1])\n",
    "    \n",
    "    return t_interior, S_interior, t_terminal, S_terminal\n",
    "\n",
    "#%% Loss function\n",
    "\n",
    "def loss(model, t_interior, S_interior, t_terminal, S_terminal):\n",
    "    ''' Compute total loss for training.\n",
    "    \n",
    "    Args:\n",
    "        model:      DGM model object\n",
    "        t_interior: sampled time points in the interior of the function's domain\n",
    "        S_interior: sampled space points in the interior of the function's domain\n",
    "        t_terminal: sampled time points at terminal point (vector of terminal times)\n",
    "        S_terminal: sampled space points at terminal time\n",
    "    ''' \n",
    "    \n",
    "    # Loss term #1: PDE\n",
    "    # compute function value and derivatives at current sampled points\n",
    "    V = model(t_interior, S_interior)\n",
    "    V_t = tf.gradients(ys=V, xs=t_interior)[0]\n",
    "    V_s = tf.gradients(ys=V, xs=S_interior)[0]\n",
    "    V_ss = tf.gradients(ys=V_s, xs=S_interior)[0]\n",
    "    diff_V = V_t + 0.5 * sigma**2 * S_interior**2 * V_ss + r * S_interior * V_s - r*V\n",
    "\n",
    "    # compute average L2-norm of differential operator\n",
    "    L1 = tf.reduce_mean(input_tensor=tf.square(diff_V)) \n",
    "    \n",
    "    # Loss term #2: boundary condition\n",
    "        # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term #3: initial/terminal condition\n",
    "    target_payoff = tf.nn.relu(S_terminal - K)\n",
    "    fitted_payoff = model(t_terminal, S_terminal)\n",
    "    \n",
    "    L3 = tf.reduce_mean( input_tensor=tf.square(fitted_payoff - target_payoff) )\n",
    "\n",
    "    return L1, L3\n",
    "\n",
    "#%% Set up network\n",
    "\n",
    "# initialize DGM model (last input: space dimension = 1)\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "# tensor placeholders (_tnsr suffix indicates tensors)\n",
    "# inputs (time, space domain interior, space domain at initial time)\n",
    "t_interior_tnsr = tf.compat.v1.placeholder(tf.float32, [None,1])\n",
    "S_interior_tnsr = tf.compat.v1.placeholder(tf.float32, [None,1])\n",
    "t_terminal_tnsr = tf.compat.v1.placeholder(tf.float32, [None,1])\n",
    "S_terminal_tnsr = tf.compat.v1.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_interior_tnsr, S_interior_tnsr, t_terminal_tnsr, S_terminal_tnsr)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "# option value function\n",
    "V = model(t_interior_tnsr, S_interior_tnsr)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "#%% Train network\n",
    "# for each sampling stage\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t_interior, S_interior, t_terminal, S_terminal = sampler(nSim_interior, nSim_terminal)\n",
    "    \n",
    "    # for a given sample, take the required number of SGD steps\n",
    "    for _ in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_interior_tnsr:t_interior, S_interior_tnsr:S_interior, t_terminal_tnsr:t_terminal, S_terminal_tnsr:S_terminal})\n",
    "    \n",
    "    print(loss, L1, L3, i)\n",
    "\n",
    "# save outout\n",
    "if saveOutput:\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    saver.save(sess, './SavedNets/' + saveName)\n",
    "\n",
    "#%% Plot results\n",
    "\n",
    "# # LaTeX rendering for text in plots\n",
    "# plt.rc('text', usetex=True)\n",
    "# plt.rc('font', family='serif')\n",
    "\n",
    "# figure options\n",
    "plt.figure()\n",
    "plt.figure(figsize = (12,10))\n",
    "\n",
    "# time values at which to examine density\n",
    "valueTimes = [t_low, T/3, 2*T/3, T]\n",
    "\n",
    "# vector of t and S values for plotting\n",
    "S_plot = np.linspace(S_low, S_high, n_plot)\n",
    "\n",
    "for i, curr_t in enumerate(valueTimes):\n",
    "    \n",
    "    # specify subplot\n",
    "    plt.subplot(2,2,i+1)\n",
    "    \n",
    "    # simulate process at current t \n",
    "    optionValue = BlackScholesCall(S_plot, K, r, sigma, curr_t)\n",
    "    \n",
    "    # compute normalized density at all x values to plot and current t value\n",
    "    t_plot = curr_t * np.ones_like(S_plot.reshape(-1,1))\n",
    "    fitted_optionValue = sess.run([V], feed_dict= {t_interior_tnsr:t_plot, S_interior_tnsr:S_plot.reshape(-1,1)})\n",
    "    \n",
    "    # plot histogram of simulated process values and overlay estimated density\n",
    "    plt.plot(S_plot, optionValue, color = 'b', label='Analytical Solution', linewidth = 3, linestyle=':')\n",
    "    plt.plot(S_plot, fitted_optionValue[0], color = 'r', label='DGM estimate')    \n",
    "    \n",
    "    # subplot options\n",
    "    plt.ylim(ymin=0.0, ymax=K)\n",
    "    plt.xlim(xmin=0.0, xmax=S_high)\n",
    "    plt.xlabel(\"Spot Price\", fontsize=15, labelpad=10)\n",
    "    plt.ylabel(\"Option Price\", fontsize=15, labelpad=20)\n",
    "    # plt.title(r\"\\boldmath{$t$}\\textbf{ = %.2f}\"%(curr_t), fontsize=18, y=1.03))\n",
    "    plt.title('Time: {}'.format(str(curr_t)))\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.grid(linestyle=':')\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.legend(loc='upper left', prop={'size': 16})\n",
    "    \n",
    "# adjust space between subplots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.4)\n",
    "\n",
    "current_path = os.path.dirname(os.path.abspath(__file__))\n",
    "if saveFigure:\n",
    "    # plt.savefig(figureName)\n",
    "    plt.savefig(os.path.join(current_path, '{}.png'.format(figureName)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d9365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
